# Archived API Ingestion Scripts

This directory contains a set of Python scripts for ingesting bibliometric and altmetric data from various academic APIs. These scripts form a data pipeline that retrieves publication data from Scopus and enriches it with additional metrics from SciVal, Altmetric, and Overton.

## Pipeline Overview

The scripts should be executed in the following order:

1. **`scopus_search_api.py`** - Retrieves publication data for specified authors from Scopus Search API
2. **`extract_scopus_doi_publication_ids.py`** - Extracts DOIs and publication IDs from Scopus results
3. Run any or all of the following (no dependency order between them):
   - **`scival_publication_metrics_api.py`** - Retrieves bibliometric metrics using publication IDs
   - **`altmetric_api.py`** - Retrieves altmetric data using DOIs
   - **`overton_api.py`** - Retrieves policy citation data using DOIs

## Common Patterns

All ingestion scripts share the following architectural patterns:

### State Management
- **Load ID System**: Each processing run generates a unique load ID (`{source}_{timestamp}`) to track related processing batches
- **State Files**: JSON files stored in `data/state/{source}/load_id_{id}/` track the progress of each processing unit (author, DOI, batch, or chunk)
- **Idempotent Storage**: Content hashing prevents duplicate file writes when reprocessing
- **Auto-Recovery**: Scripts detect incomplete processing and resume from the last successful checkpoint

### Directory Structure
- **Date Partitioning**: Raw data and manifests are stored in `YYYYMM/DD` directories
- **Cache Management**: HTTP request caching reduces redundant API calls
- **Organised Logging**: Centralised logs in `logs/` directory

### Error Handling
- **Exponential Backoff**: Configurable retry attempts with increasing wait times
- **Error Tracking**: State files record errors with timestamps for debugging
- **Graceful Degradation**: Processing continues with partial results when possible

### Configuration
- **YAML Configuration**: API endpoints, retry settings, and default parameters are externalised in `config/apis/`
- **Environment Variables**: API keys and institutional tokens are stored securely as environment variables

### Manifest Generation
- **Processing Metadata**: Each run generates a manifest file containing processing statistics and file mappings
- **Coverage Metrics**: Manifests include success rates, error counts, and data quality indicators

## Script-Specific Details

### 1. Scopus Search API (`scopus_search_api.py`)

**Purpose**: Foundation script that retrieves publication data for specified authors from the Scopus Search API.

**Processing Unit**: Individual authors (identified by Scopus AU-ID)

**Key Features**:
- Pagination handling for authors with large publication counts
- Extracts DOIs and EIDs from search results
- Validates all files before marking an author as complete

**Usage**:
```bash
python scopus_search_api.py --input author_ids.txt --load-id scopus_20250101120000
```

**Command Line Options**:
- `--input, -i`: File containing Scopus AU-IDs, one per line
- `--output-dir, -o`: Base output directory for raw JSON files
- `--manifest-dir, -m`: Base directory for manifest files
- `--state-dir, -s`: Directory for state files
- `--retries, -r`: Maximum number of retry attempts
- `--force-reprocess, -f`: Force reprocessing of all authors, ignoring existing state
- `--rebuild-manifest`: Rebuild manifest from existing state and files
- `--include-dois`: Include full DOI lists in manifest file
- `--single-author`: Process only a single author ID
- `--load-id, -l`: Specify a load_id to use for continuing a previous run

### 2. Extract Scopus DOI and Publication IDs (`extract_scopus_doi_publication_ids.py`)

**Purpose**: Utility script that extracts unique DOIs and Scopus publication IDs from the JSON files generated by the Scopus Search API.

**Key Features**:
- Transforms EIDs (format: `2-s2.0-{id}`) to Scopus publication IDs by removing the prefix
- Generates two output files:
  - `scopus_publication_ids_{date}.txt` - For SciVal API
  - `scopus_generated_dois_{date}.txt` - For Altmetric and Overton APIs
- Handles both metadata-wrapped and direct JSON structures

**Usage**:
```bash
python extract_scopus_doi_publication_ids.py --date 20250101
```

**Command Line Options**:
- `--input-dir, -i`: Directory containing the JSON files (default: auto-detect based on date)
- `--output-dir, -o`: Base output directory for extracted identifiers
- `--date, -d`: Optional specific date to process in YYYYMMDD format

### 3. SciVal Publication Metrics API (`scival_publication_metrics_api.py`)

**Purpose**: Retrieves bibliometric metrics for publications using Scopus publication IDs.

**Processing Unit**: Chunks of up to 25 publication IDs per request (API limitation)

**Key Differences from Scopus Script**:
- Processes publication IDs in fixed-size chunks rather than paginating through individual author results
- Requests multiple metrics simultaneously (configurable in YAML)
- No pagination within chunks as results are returned in a single response per chunk

**Usage**:
```bash
python scival_publication_metrics_api.py --load-id scival_20250101120000
```

**Command Line Options**:
- `--input, -i`: Path to file containing Scopus publication IDs
- `--output-dir, -o`: Base output directory for raw JSON files
- `--manifest-dir, -m`: Base directory for manifest files
- `--state-dir, -s`: Directory for state files
- `--retries, -r`: Maximum number of retry attempts
- `--force-reprocess, -f`: Force reprocessing of all chunks
- `--load-id, -l`: Load ID to resume processing

### 4. Altmetric API (`altmetric_api.py`)

**Purpose**: Retrieves altmetric data (social media mentions, blog posts, news coverage) for publications using DOIs.

**Processing Unit**: Individual DOIs processed sequentially

**Key Differences from Scopus Script**:
- Processes DOIs one at a time rather than in batches or with pagination
- Handles 404 responses gracefully (DOI not found in Altmetric is expected for many publications)
- Rate limiting controlled by `requests_per_second` parameter
- Extracts specific metrics: Bluesky mentions, Twitter mentions, and Altmetric score

**Usage**:
```bash
python altmetric_api.py --rate 50 --load-id altmetric_20250101120000
```

**Command Line Options**:
- `--input, -i`: Path to file containing DOIs
- `--output-dir, -o`: Base output directory for raw JSON files
- `--manifest-dir, -m`: Base directory for manifest files
- `--state-dir, -s`: Directory for state files
- `--retries, -r`: Maximum number of retry attempts
- `--rate`: Requests per second (default: 50)
- `--force-reprocess, -f`: Force reprocessing of all DOIs
- `--load-id, -l`: Load ID to resume processing

### 5. Overton API (`overton_api.py`)

**Purpose**: Retrieves policy document citations for publications using DOIs.

**Processing Unit**: Batches of up to 25 DOIs per set (API limitation)

**Key Differences from Scopus Script**:
- **Two-Step Process**: First generates a set ID from a batch of DOIs, then retrieves policy documents for that set
- **Pagination Within Batches**: After generating a set, the script paginates through policy document results
- **Set ID Persistence**: Set IDs are stored in state files to avoid regenerating them on recovery
- **Complex Result Structure**: Extracts policy document IDs and tracks which DOIs are cited in which policy documents

**Usage**:
```bash
python overton_api.py --load-id overton_20250101120000
```

**Command Line Options**:
- `--input, -i`: Path to file containing DOIs
- `--output-dir, -o`: Base output directory for raw JSON files
- `--manifest-dir, -m`: Base directory for manifest files
- `--state-dir, -s`: Directory for state files
- `--retries, -r`: Maximum number of retry attempts
- `--force-reprocess, -f`: Force reprocessing of all batches
- `--load-id, -l`: Load ID to resume processing

## Environment Variables

The following environment variables must be set before running the scripts:

- `SCOPUS_API_KEY` - API key for Scopus and SciVal APIs
- `SCOPUS_INST_ID` - Institutional token for Scopus and SciVal APIs
- `ALTMETRIC_API_KEY` - API key for Altmetric API
- `OVERTON_API_KEY` - API key for Overton API

## Configuration Files

Each API has a corresponding YAML configuration file in `config/apis/`:

- `scopus.yaml` - Scopus Search API configuration
- `scival.yaml` - SciVal Metrics API configuration
- `altmetric.yaml` - Altmetric API configuration
- `overton.yaml` - Overton API configuration

Configuration files contain:
- Base URLs and endpoints
- Default query parameters
- Retry and rate limiting settings
- Cache configuration
- Requested metrics (for SciVal)

## Output Structure

```
data/
├── raw/
│   ├── api_input/
│   │   └── YYYYMM/
│   │       ├── scopus_publication_ids_YYYYMMDD.txt
│   │       └── scopus_generated_dois_YYYYMMDD.txt
│   ├── elsevier/
│   │   ├── scopus/
│   │   │   └── YYYYMM/DD/
│   │   │       └── scopus_search_author_{au_id}_page{n}_{date}.json
│   │   └── scival/
│   │       └── YYYYMM/DD/
│   │           └── scival_metrics_chunk{n}_{date}.json
│   ├── altmetric/
│   │   └── YYYYMM/DD/
│   │       └── altmetric_doi_{doi}_{date}.json
│   ├── overton/
│   │   └── YYYYMM/DD/
│   │       └── overton_raw_batch_{n}_{date}.json
│   └── manifest/
│       └── YYYYMM/DD/
│           ├── scopus_manifest_YYYYMMDD.json
│           ├── scival_manifest_YYYYMMDD.json
│           ├── altmetric_manifest_YYYYMMDD.json
│           └── overton_manifest_YYYYMMDD.json
├── state/
│   ├── scopus/
│   │   └── load_id_{id}/
│   │       └── author_{au_id}_state.json
│   ├── scival/
│   │   └── load_id_{id}/
│   │       └── chunk_{n}_state.json
│   ├── altmetric/
│   │   └── load_id_{id}/
│   │       └── doi_{doi}_state.json
│   └── overton/
│       └── load_id_{id}/
│           └── batch_{n}_state.json
└── cache/
    ├── elsevier/
    │   ├── scopus/
    │   └── scival/
    ├── altmetric/
    └── overton/
```

## Recovery and Resumption

All scripts support automatic recovery from failures:

1. **Resume with Existing Load ID**: Use the `--load-id` flag with the ID from a previous run
2. **Automatic State Detection**: Scripts automatically detect incomplete processing units and resume from the last successful checkpoint
3. **File Validation**: Existing files are validated before skipping to ensure data integrity
4. **Force Reprocessing**: Use `--force-reprocess` flag to ignore existing state and reprocess all data

Example resuming from a failure:
```bash
# Original run failed
python scopus_search_api.py --input authors.txt

# Resume using the load_id from the log output
python scopus_search_api.py --input authors.txt --load-id scopus_20250101120000
```

## Logging

All scripts generate detailed logs in the `logs/` directory:

- `scopus_author_publication_ingestion.log`
- `scival_publication_metrics_ingestion.log`
- `altmetric_ingestion.log`
- `overton_policy_ingestion.log`
- `scopus_doi_and_pubid_extraction.log`

Logs include:
- Processing progress with percentage completion
- API rate limit information
- Error details with timestamps
- File operations (creation, validation, skipping)
- State updates

## Best Practises

1. **Monitor Rate Limits**: Check logs for API rate limit warnings and adjust `requests_per_second` in configuration files if needed
2. **Validate Outputs**: Review manifest files after each run to check coverage rates and error counts
3. **Use Load IDs**: Always note the load ID from the log output for potential recovery
4. **Incremental Processing**: For large datasets, consider processing in smaller batches using the `--single-author` flag (Scopus) or splitting input files
5. **Environment Setup**: Verify all environment variables are set before starting a pipeline run